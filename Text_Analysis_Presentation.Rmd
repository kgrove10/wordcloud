---
title: "Visualizing and Analyzing Text Data"
subtitle: "with tidytext and ggwordcloud"
author: "Kivalina Grove"
date: "02/04/2019"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

background-image: url(LegoGradStudent.jpg)
class: center, bottom, inverse

"Enjoying his work, the grad student solemnly ponders whether he has fallen victim to Stockholm syndrome. #gradschool"

Image credit: [Lego Grad Student](https://legogradstudent.tumblr.com/image/177771200086)

---

## Getting Twitter Data: rtweet package

Install the [**rtweet**](https://cran.r-project.org/web/packages/rtweet/rtweet.pdf) package. 

Now we can easily search and pull tweets directly from Twitter:

```{r eval = FALSE, tidy = FALSE}
#can search hashtags directly
data <- search_tweets("#gradschool", n = 18000, include_rts = FALSE)

#spaces between words operate like boolean "AND" operator
data <- search_tweets("grad school", n = 18000, include_rts = FALSE)

#use "OR" to search for tweets with one of a list of words
data <- search_tweets("grad OR school", n = 18000, include_rts = FALSE)
```

Some things to note:

- Only returns recent tweets: from past 6-9 days (due to Twitter's Search API)
- Limit of 18,000 tweets on a single call, but can set *retryonratelimit = TRUE* to get more.
- Can choose to include retweets with *include_rts = TRUE*

---

## Getting Data in a Tidy Text Format

Before we can analyze our data, we need to get it into a tidy text format: a table where there is **one token per row**, per [Text Mining With R](https://www.tidytextmining.com). 

A token is a meaningful unit of text we are interested in for analysis (word, phrase, sentence, paragraph). 

```{r include = FALSE}
library(tidyr)
library(dplyr)
library(tidytext)
library(ggplot2)
library(ggwordcloud)
library(rio)
gs_data <- readRDS("gs_data.rds")
```

Let's start by pulling out the columns we are interested in:
```{r gsdataRead, message=FALSE, warning=FALSE}
gs_data <- gs_data %>% 
  select(screen_name, created_at, text, hashtags)
head(gs_data)
```

---
## Getting Data in a Tidy Text Format

Next, we can use the **unnest_tokens()** function (from package [**tidytext**](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html)) to split the words in the "text" column apart into a new column called "word" where each word is on its own row (tidytext format!). 

```{r tidytext}
gs_words <- gs_data %>%
  unnest_tokens(word, text, token = "words")

head(gs_words, n = 4)
```

- Note that we could use different tokens here: built in options for "characters", "words", "ngrams", "sentences", "lines" etc. 
- "ngrams" are adjacent words, use argument *n = 2* to select word pairs, for example. 
---
## Word Counts

Now that our data is in a tidytext format, we can count the number of occurrences of each word. 

```{r counts1}
gs_counts <- gs_words %>%
  count(word, sort = TRUE)

head(gs_counts, n = 10)
```

---

## Quick Graph of Word Counts (n > 400)

```{r gs_graph1, echo = FALSE, fig.height= 6, fig.width = 8}
gs_counts %>%
  filter(n > 400) %>%
  mutate(word = reorder(as.factor(word), n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col(alpha = 0.7, fill = "#4286f4", color = "white") +
  coord_flip() +
  theme_light() +
  theme(text = element_text(size = 20))
```

Issue: seeing lots of common words (and hyperlinks) - not very informative! 

---

## Stop Words

We can remove these common words ("a", "the", "and", etc.) using an anti_join with the *stop_words* dataset from the **tidytext** package. 

```{r stopwordsremove, eval = FALSE}
gs_words <- gs_words %>%
  anti_join(stop_words)
```

To remove the "t.co" and "https" words we are seeing (remnants of twitter links!), we can just use bind_rows to add them to our stop_words list. 
```{r customstopwords}
c_stop_words <- bind_rows(data_frame(word = c("t.co", "https", "iâ€™m"), 
                                          lexicon = c("custom", "custom", "custom")), 
                                          stop_words)

gs_words <- gs_words %>%
  anti_join(c_stop_words)
```

---

## Graph Without Stop Words

Now when we graph the word count, it is much more informative. 

```{r wordcountgraph2, echo = FALSE, fig.height = 6, fig.width = 8}
gs_counts <- gs_words %>%
  count(word, sort = TRUE)

gs_counts %>%
  filter(n > 70) %>%
  mutate(word = reorder(as.factor(word), n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col(alpha = 0.7, fill = "#4286f4", color = "white") +
  coord_flip() +
  theme_light() +
  theme(text = element_text(size = 20))
```

---

## A Slight Tangent: Other Stop Words

You can customize the stop_words list, but for some data, you might want an entirely different stop_words list.  For example, say you were interested in mapping word frequency in Shakespeare's "The Comedy of Errors".  

Using the [**gutenbergr**](https://cran.r-project.org/web/packages/gutenbergr/vignettes/intro.html) package, we can easily import text available through Project Gutenberg.  Using the *gutenberg_works()* function allows us to search by many parameters (author, title, etc.) only among those texts that are in English, can be downloaded, etc.  Using the ID#, we can then download the text. 

```{r gutenberg, message=FALSE, warning=FALSE}
library(gutenbergr)
gutenberg_works(title == "The Comedy of Errors")

comedy_errors <- gutenberg_download(1504)
```

---
## A Slight Tangent: Other Stop Words

Replicating what we did before with the twitter data, we can then use the *unnest_tokens()* function, *anti_join()* with stop_words, and then count and sort. 

```{r comedy_errors_count, message=FALSE, warning=FALSE}
ce_words <- comedy_errors %>% unnest_tokens(word, text) %>%
  anti_join(stop_words) %>% count(word, sort = TRUE)

head(ce_words, n = 8)
```
We can see that there are still several common words ("thee", "thou") that weren't removed by the *anti_join()* with stop_words.  

---
## Elizabethan Stop Words

Luckily for us, [Bryan Bumgardner](http://bryanbumgardner.com/elizabethan-stop-words-for-nlp/) has created a list of [Elizabethan Stop Words](https://github.com/BryanBumgardner/gutenburg_nlp/blob/master/stopwords_elizabethan) for this very use. 

Thus, we can easily download Bryan's elizabethan_stop_words file, and use that to remove common words and create a WordCloud for "The Comedy of Errors".

```{r improvedShakespeare}
eliz_stop_words <- import("stopwords_elizabethan.html", class = "tbl_df")
eliz_stop_words$V1[127:150]

ce_words <- comedy_errors %>% 
  unnest_tokens(word, text) %>%
  anti_join(eliz_stop_words, by = c("word" = "V1")) %>% 
  count(word, sort = TRUE)
```

---

## The Comedy of Errors Word Cloud

```{r wordCloudShakes, echo = FALSE, fig.align = "center"}
ce_angle <- ce_words %>%
  mutate(angle = 90 * sample(c(0, 1), n(), 
                             replace = TRUE, 
                             prob = c(50, 50)))

ce_angle <- ce_angle %>%
  filter(n > 25)

ggplot(ce_angle, aes(label = word, size = n, angle = angle, color = n)) +
  geom_text_wordcloud_area(rm_outside = TRUE) +
  scale_size_area(max_size = 30) +
  theme_minimal() +
  scale_color_distiller(palette = "RdYlGn")
```

---

## Back on Track: Assembling a Word Cloud

Now that we have the counts, we can use the package [**ggwordcloud**](https://cran.r-project.org/web/packages/ggwordcloud/vignettes/ggwordcloud.html) to create a wordcloud of the most common words. 

```{r wordcloud1, eval = FALSE}
#filter to remove really large count for gradschool, and words with small counts
gs_filter <- gs_counts %>%
  filter(n > 40 & n < 1000)

#create wordcloud
set.seed(12)
ggplot(gs_filter, aes(label = word, size = n)) +
  geom_text_wordcloud_area(rm_outside = TRUE) +
  scale_size_area(max_size = 25) +
  theme_minimal() 
```

- Note: use *set.seed()* command to avoid random fluctuation in word placement within cloud.
- *scale_size_area()* sets true proportionality (so the words are scaled proportional to the value), and allows for better font size control. 
- set *rm_outside = TRUE* to avoid overlapping words (removes words that don't fit).

---
## Basic Word Cloud


```{r wordcloudexecute, echo = FALSE, fig.align = "center"}
#filter to remove really large count for gradschool, and words with small counts
gs_filter <- gs_counts %>%
  filter(n > 40 & n < 1000)

#create wordcloud
set.seed(12)
ggplot(gs_filter, aes(label = word, size = n)) +
  geom_text_wordcloud_area(rm_outside = TRUE) +
  scale_size_area(max_size = 25) +
  theme_minimal() 
```

---
## Some Wordcloud Options: Angles

To rotate some words, set the *angle* aesthetic.  Here, example of having 50% of the words randomly be turned 90 degrees. 

```{r angled}
gs_filter_angle <- gs_filter %>%
  mutate(angle = 90 * sample(c(0, 1), n(), 
                             replace = TRUE, 
                             prob = c(50, 50)))

wc1 <- ggplot(gs_filter_angle, aes(label = word, 
                                   size = n, 
                                   angle = angle)) +
  geom_text_wordcloud_area(rm_outside = TRUE) +
  scale_size_area(max_size = 25) +
  theme_minimal() 
```

---

##Angled Word Cloud

```{r angledprint, echo=FALSE, fig.align="center", message=FALSE, warning=FALSE}
wc1
```

---

## Some Wordcloud Options: Shaped Clouds

You can change the shape the wordcloud is presented in.  Default shape is a circle, but you can also use other shapes in the *shape* option within the *geom_text_wordcloud_area()* function.

- cardioid
- diamond 
- square 
- triangle-forward
- triangle-upright
- pentagon
- star   

```{r shapedCloud}
wc2 <- ggplot(gs_filter, aes(label = word, size = n)) +
  geom_text_wordcloud_area(rm_outside = TRUE, shape = "diamond") +
  scale_size_area(max_size = 20) +
  theme_minimal() 
```

---

## Shaped WordClouds: Diamond

```{r shapedCloudprint, echo=FALSE, fig.align="center", message=FALSE, warning=FALSE}
wc2
```

---

## Shaped WordClouds: Star

```{r shapedCloudprintStar, echo=FALSE, fig.align="center", message=FALSE, warning=FALSE}
set.seed(85)
ggplot(gs_filter, aes(label = word, size = n)) +
  geom_text_wordcloud_area(rm_outside = TRUE, shape = "star") +
  scale_size_area(max_size = 20) +
  theme_minimal() 
```

---

##Some Word Cloud Options: Color

You can assign colors to the words in a cloud both randomly, and based on a value. 

Random color assignment: 
```{r color}
color <- factor(sample.int(10, nrow(gs_filter), replace = TRUE))

wc3 <- ggplot(gs_filter, aes(label = word, size = n, color = color)) +
  geom_text_wordcloud_area(rm_outside = TRUE) +
  scale_size_area(max_size = 25) +
  theme_minimal() 
```

Color assignment by the value of n:
```{r color2}
wc4 <- ggplot(gs_filter, aes(label = word, size = n, color = n)) +
  geom_text_wordcloud_area(rm_outside = TRUE) +
  scale_size_area(max_size = 25) +
  theme_minimal() +
  scale_color_distiller(palette = "Spectral")
```

---
## Word Cloud in (Random) Color

```{r colorprint, echo = FALSE, fig.align = "center"}
wc3
```

---

## Word Cloud in Color (by n)

```{r colorprint2, echo = FALSE, fig.align = "center"}
wc4
```

---

## Sentiment Analysis

We can also filter the words in our cloud according to their sentiment using the sentiment lexicons in the **sentiments** dataset within the **tidytext** package. 

I'm going to use the "bing" lexicon, which categorizes words into positive or negative categories, and then we can use an *inner_join()* to combine this sentiment information with our tidytext data. 

```{r sentimentCloud, message=FALSE, warning=FALSE}
gs_words <- gs_words %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE)

set.seed(15)
sentcloud <- gs_words %>% 
  group_by(sentiment) %>% 
  top_n(15) %>%
  ggplot(aes(label = word, size = n, color = sentiment)) +
  geom_text_wordcloud_area(rm_outside = TRUE) +
  scale_size_area(max_size = 30) +
  theme_minimal()
```

---
## Sentiment Analysis Word Cloud
```{r sentimentCloudprint, echo=FALSE, fig.align="center", message=FALSE, warning=FALSE}
sentcloud
```


